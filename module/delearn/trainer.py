#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
__doc__=r"""
:py:mod:`delearn/trainer.py`
"""
#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
__all__ = [

     'Trainer', 'Callback', 'TrainingCallback',

]
#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
import torch as tt
import torch.autograd as ag
from tqdm import tqdm
#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

# NOTE: forward signature is forward(module, inputs)


class Trainer:
    r"""Provides functional training methods"""

    @staticmethod
    @tt.no_grad()
    def predict_batch(inputs, labels, forward, module, lossf):
        r"""Predict on given batch of data (input, labels)
        
        :param forward: callable like forward(module, inputs)->outputs
        :param module: a Module object that holds parameters
        :param inputs: the inputs to be passed to forward to get predictions
        :param labels: the labels to match predicts with to calculate loss
        :param lossf: callable like lossf(predictions, labels)->loss

        Returns: loss, predictions
        """
        preds = forward(module, inputs)
        loss = lossf(preds, labels).item()
        return loss, preds
    
    @staticmethod
    def train_batch(inputs, labels, forward, module, lossf, optimf, create_graph=False):
        r"""Trains on given batch of data (input, labels)
        
        :param forward: callable like forward(module, inputs)->outputs
        :param module: a Module object that holds parameters
        :param inputs: the inputs to be passed to forward to get predictions
        :param labels: the labels to match predicts with to calculate loss
        :param lossf: callable like lossf(predictions, labels)->loss
        :param optimf: callable like optimf(module, grads, create_graph)->module
        :param create_graph: autograd argument for the grads of module wrt loss, if True, the optim does not use no_grad().

        Returns: loss, module (module will be a new module if create_graph is True)
        """

        preds = forward(module, inputs)
        #print(f'Loss:{x.shape=}, {ypred.shape=}, {y.shape=}')
        loss = lossf(preds, labels)
        grads = ag.grad(loss, module.values(), create_graph=create_graph)
        module = optimf(module, grads, create_graph)
        lossval = loss.item()
        return lossval, module
    
    @staticmethod
    def train_dataset(forward, module, lossf, optimf, dataf, n, batch_mode, create_graph=False, callback=None):  
        r"""Trains on given Iterator/Dataloader generated by dataf on each epoch 
        
        :param forward: callable like forward(module, inputs)->outputs
        :param module: a Module object that holds parameters
        :param lossf: callable like lossf(predictions, labels)->loss
        :param optimf: callable like optimf(module, grads, create_graph)->module
        :param create_graph: autograd argument for the grads of module wrt loss, if True, the optim does not use no_grad().
        :param dataf: callable like dataf(epoch)->Iterator/Dataloader
        :param n: no of epochs/batches
        :param batch_mode: if True, n=epochs (trains n times on full data) else n=batches (trains n batches from data)
        :param callback: a callback object

        Returns: callback
        """
        if callback is None: callback = Callback()

        callback.on_train_begin(batch_mode, n, optimf)  

        if batch_mode:
            epoch, di = -1, iter([]) # to raise StopIteration on first call
            for batch in tqdm(range(n)): 
                try:
                    x,y = next(di)
                except StopIteration:
                    if epoch>-1: callback.on_epoch_end(epoch)
                    epoch+=1
                    callback.on_epoch_begin(epoch)
                    di = iter(dataf(epoch)) 
                    x,y = next(di)
                callback.on_batch_begin(batch, x, y)
                batch_loss, module = __class__.train_batch(x, y, forward, module, lossf, optimf, create_graph=create_graph)
                callback.on_batch_end(batch, batch_loss, module) 
        else:
            for epoch in tqdm(range(n)): 
                callback.on_epoch_begin(epoch)
                for batch,(x,y) in enumerate(dataf(epoch), 0):
                    callback.on_batch_begin(batch, x, y)
                    batch_loss, module = __class__.train_batch(x, y, forward, module, lossf, optimf, create_graph=create_graph)
                    callback.on_batch_end(batch, batch_loss, module) 
                callback.on_epoch_end(epoch) 

        callback.on_train_end(n)  


        return callback #train_losses
        # ============================================================================

class Callback(object):
    r"""Defines a callback object for training loop"""

    def __init__(self) -> None: super().__init__()

    def on_train_begin(self, batch_mode, n, optimf): 
        pass #  - setup optimizer, initialize loss history

    def on_epoch_begin(self, epoch): 
        pass # record epoch level activity, dataset

    def on_batch_begin(self, batch, x, y): 
        pass # batch id and data

    def on_batch_end(self, batch, batch_loss, module): 
        pass # record batch loss, (if create_graph=True, store module which are new parameters)

    def on_epoch_end(self, epoch): 
        pass # record epoch level activity, dataset

    def on_train_end(self, n): 
        pass # - reset optimizer, display results

class TrainingCallback(Callback):
    def __init__(self, val_data=None, forward=None, lossf=None) -> None: 
        super().__init__()
        self.val_data = val_data
        self.val_x, self.val_y = self.val_data
        self.forward=forward
        self.lossf=lossf
        self.epoch_losses = []
        self.batch_losses = []
        self.val_losses = []
        self.val_preds = []

    def validate(self,  module):
        loss, preds = Trainer.predict_batch( self.val_x,  self.val_y,  self.forward, module,  self.lossf)
        self.val_losses.append(loss)
        self.val_preds.append(preds)


    def clear_val(self):
        self.val_losses.clear()
        self.val_preds.clear()

    def clear_train(self):
        self.epoch_losses.clear()
        self.batch_losses.clear()

    def clear(self):
        self.clear_train()
        self.clear_val()

    def on_train_begin(self, batch_mode, n, optimf): 
        self._epoch_losses = []
        self.batch_mode=batch_mode
        self.n = n

    def on_epoch_begin(self, epoch): 
        self._batch_losses = []
        self.module = None

    def on_batch_begin(self, batch, x, y): 
        pass # batch id and data

    def on_batch_end(self, batch, batch_loss, module):
        self._batch_losses.append(batch_loss)
        self.module = module

    def on_epoch_end(self, epoch): 
        losses = tt.tensor(self._batch_losses)
        self.batch_losses.append(losses)
        self._epoch_losses.append(tt.mean(losses).item())
        del self._batch_losses
        if self.val_data: self.validate(self.module)
        #self.module = None

    def on_train_end(self, n): 
        self.epoch_losses.append(tt.tensor(self._epoch_losses))
        del self._epoch_losses

    def plot_results(self, figsize=(12, 12), dpi=None, save=None):
        import matplotlib.pyplot as plt
        
        
        fig,ax = plt.subplots(2, 1, figsize=figsize, dpi=dpi, constrained_layout=True)
        ax_batch, ax_epoch = ax[1], ax[0]

        ax_batch.set_title('Batch Loss')
        ax_epoch.set_title('Epoch Loss')

        batch_losses= tt.hstack(self.batch_losses) #.flatten()
        ax_batch.plot(batch_losses, linewidth=0.75, color='tab:grey', label='batch_loss')

        epoch_losses= tt.hstack(self.epoch_losses) #.flatten()
        ax_epoch.plot(epoch_losses, linewidth=0.75, color='tab:blue', label='mean_loss')
        if self.val_data: ax_epoch.plot(self.val_losses, linewidth=0.75, color='tab:red', label='val_loss')

        ax_batch.legend()
        ax_epoch.legend()
        if save:
            fig.savefig(f'{save}')
        else:
            plt.show()
        plt.close()